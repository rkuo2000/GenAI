## Text-to-Text (LLMs)

### Python sample codes
* `bailong.py`
* `breeze.py`
* `gemma-1.1.py`
* `llama-2.py`
* `llama-3.py`
* `mamba.py` # for -130M, -370M, -790M, -1B, -3B, -3B-slimpj
* `mistral.py`
* `mobilellama.py`
* `phi-2.py`
* `phi-3.py`
* `qwen1.5.py`
* `starling-lm.py`
* `taide-lx.py`
* `taide-lx-llama3.py`
* `taiwan-llm.py`
* `tinygllama.py`
* `vicuna.py`

---
### LLM prompting
* `python llm_prompting.py`
* [colab_LLM_prompting.ipynb](https://github.com/rkuo2000/GenAI/blob/main/Text-to-Text/colab_LLM_prompting.ipynb)
  
---
### local-LLM Server & Client
* `python llm_server.py` (on GPU)
* `python llm_client.py` (on PC)
* `python post_text.py` (on PC)

---
### Ollama Server & Client
* `ollama run llama3` (on GPU)
* `python ollama_client.py` (on PC)
  
---
### Colab's LLM Server & Client
* [colab_pyNgrok_LLM_Server.ipynb](https://github.com/rkuo2000/GenAI/blob/main/Text-to-Text/colab_pyNgrok_LLM_Server.ipynb) (on Colab T4)<br>
![](https://github.com/rkuo2000/GenAI/blob/main/assets/pyngrok_LLM_Server.png?raw=true)
* [post-text.py](https://github.com/rkuo2000/GenAI/blob/main/Text-to-Text/post_text.py) (on your PC)<br>
![](https://github.com/rkuo2000/GenAI/blob/main/assets/pyngrok_post_text.png?raw=true)
